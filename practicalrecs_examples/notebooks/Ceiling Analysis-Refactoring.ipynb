{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pybloomfilter as pbf\n",
    "import torch as th\n",
    "from pytorch_lightning import seed_everything\n",
    "from ranking_metrics_torch.cumulative_gain import ndcg_at\n",
    "from ranking_metrics_torch.precision_recall import precision_at, recall_at\n",
    "from torch_factorization_models.movielens import MovielensDataModule\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from practicalrecs_examples.ann_search import *\n",
    "from practicalrecs_examples.dithering import *\n",
    "from practicalrecs_examples.evaluation import *\n",
    "from practicalrecs_examples.filtering import *\n",
    "from practicalrecs_examples.matrix_factorization import *\n",
    "from practicalrecs_examples.notebooks.utils import *\n",
    "from practicalrecs_examples.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for troubleshooting CUDA errors\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same seed used to create splits in training\n",
    "random_seed = seed_everything(42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA device id\n",
    "# gpu_id = th.cuda.device_count() - 1\n",
    "gpu_id = 1\n",
    "device_id = f\"cuda:{gpu_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default parameters\n",
    "dithering_eps = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_module = MovielensDataModule(\"../datasets/ml-25m/\", batch_size=512)\n",
    "movielens_module.setup()\n",
    "\n",
    "if th.cuda.is_available():\n",
    "    movielens_module.dataset.to_(device=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = movielens_module.val_dataloader(by_user=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = movielens_module.train_dataloader(by_user=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Evaluation Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness = EvaluationHarness(\n",
    "    num_candidates=250,\n",
    "    num_recs=100,\n",
    "    use_cuda=th.cuda.is_available(),\n",
    "    gpu_id=gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Bloom Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27215b8fb538460788a2704302536f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=162342.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filters = harness.artifacts.build_bloom_filters(\n",
    "    \"cap10-fp0.1\",\n",
    "    tqdm(train_dataloader.dataset),\n",
    "    expected_items=10,\n",
    "    fp_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model With Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.artifacts.load_model(\"bce\", \"../models/celestial-bee-469-bce.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Raw Model-Only Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-09100d26e2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mharness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bce\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/practicalrecs-examples/practicalrecs_examples/notebooks/utils.py\u001b[0m in \u001b[0;36mprint_metrics\u001b[0;34m(metrics)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Precision: {metrics['precision']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recall: {metrics['recall']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NDCG: {metrics['ndcg']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'precision'"
     ]
    }
   ],
   "source": [
    "model_metrics = harness.evaluate_model(\"bce\", val_dataloader)\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': tensor([0.5000, 0.3333, 0.2500,  ..., 0.7500, 0.4000, 0.3333], device='cuda:1',\n",
       "        dtype=torch.float64),\n",
       " 'ndcg': tensor([0.2522, 0.2204, 0.1789,  ..., 0.2632, 0.3984, 0.1574], device='cuda:1',\n",
       "        dtype=torch.float64)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: nan\n",
      "Ndcg: nan\n"
     ]
    }
   ],
   "source": [
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Nearest Neighbor Search Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.artifacts.build_ann_indices(\"bce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate The End-To-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.pipelines.create_template(\n",
    "    \"bce\", \"base\",\n",
    "    RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            ANNSearch(),\n",
    "        ],\n",
    "        filtering = [\n",
    "            BloomFilter(),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            MatrixFactorizationScoring(),\n",
    "        ],\n",
    "        ordering = [\n",
    "            DitheredOrdering(epsilon=dithering_eps),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.pipelines.create_pipeline(\"base\")\n",
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"base\",\n",
    "    model=\"bce\",\n",
    "    index=\"approx\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader)\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Idealized Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"ideal\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            UserEmbeddingFetcher(),\n",
    "            IdealizedANNSearch(val_dataloader.dataset),\n",
    "        ],\n",
    "        filtering = [\n",
    "            IdealizedFilter(train_dataloader.dataset),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            IdealizedMatrixFactorizationScoring(val_dataloader.dataset),\n",
    "        ],\n",
    "        ordering = []\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"ideal\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Idealized Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned user embedding, exact NN search, idealized results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"ideal-retrieval\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            UserEmbeddingFetcher(),\n",
    "            IdealizedANNSearch(val_dataloader.dataset),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"ideal-retrieval\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Idealized Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"ideal-filtering\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        filtering = [\n",
    "            IdealizedFilter(train_dataloader.dataset),\n",
    "            CandidatePadding(),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"ideal-filtering\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Idealized Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"ideal-scoring\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            IdealizedMatrixFactorizationScoring(val_dataloader.dataset),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"ideal-scoring\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Idealized Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order descending by score (omitting dithering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"ideal-ordering\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(ordering = [])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"ideal-ordering\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned user embedding, exact NN search, no idealization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"learned-vector-retrieval\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            UserEmbeddingFetcher(),\n",
    "            ANNSearch(),\n",
    "            UserAvgEmbeddingFetcher()\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"learned-vector-retrieval\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned user embedding, approx NN search, no idealization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"learned-vector-retrieval\",\n",
    "    model=\"bce\",\n",
    "    index=\"approx\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged user embedding, exact NN search, no idealization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"avg-vector-retrieval\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            ANNSearch(),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"avg-vector-retrieval\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged user embedding, approx NN search, no idealization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"avg-vector-retrieval\",\n",
    "    model=\"bce\",\n",
    "    index=\"approx\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Item embeddings, exact NN search, no idealization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"item-vectors-retrieval\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"item-vectors-retrieval\",\n",
    "    model=\"bce\",\n",
    "    index=\"exact\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item embeddings, approx NN search, no idealization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    pipeline=\"item-vectors-retrieval\",\n",
    "    model=\"bce\",\n",
    "    index=\"approx\",\n",
    "    filters=\"cap10-fp0.1\",\n",
    "    train=train_dataloader,\n",
    "    val=tqdm(val_dataloader),\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New baseline for further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_template(\"improved-retrieval\", template_name=\"base\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(),\n",
    "        ],\n",
    "        filtering = [\n",
    "            BloomFilter(),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            MatrixFactorizationScoring(),\n",
    "        ],\n",
    "        ordering = [\n",
    "            DitheredOrdering(epsilon=dithering_eps),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_idealized_stages(model_name, base_name):\n",
    "    stage_names = [\"retrieval\", \"filtering\", \"scoring\", \"ordering\"]\n",
    "    metrics = {}\n",
    "    \n",
    "    for stage_name in stage_names:\n",
    "        ideal_name = f\"ideal-{stage_name}\"\n",
    "        combined_name = f\"{base_name}-with-{ideal_name}\"\n",
    "        \n",
    "        harness.pipelines[combined_name] = \\\n",
    "            harness.templates[base_name].build(\n",
    "                overrides=harness.stages[ideal_name]\n",
    "            )\n",
    "        \n",
    "        harness.metrics[combined_name] = \\\n",
    "            harness.models[model_name].compute_ranking_metrics(\n",
    "                build_prediction_fn(harness.pipelines[combined_name], train_dataloader),\n",
    "                tqdm(val_dataloader),\n",
    "                harness.num_recs\n",
    "            )\n",
    "        \n",
    "        metrics[stage_name] = harness.metrics[combined_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.evaluate_idealized_stages(\"improved-retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage_name in [\"retrieval\", \"filtering\", \"scoring\", \"ordering\"]:\n",
    "        print(f\"With idealized {stage_name}\")\n",
    "        print(\"==============================\")\n",
    "        print_metrics(harness.metrics[\"bce\"][f\"improved-retrieval-with-ideal-{stage_name}\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Improved Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_filter_params(model, template, capacities, error_rates):\n",
    "    filter_metrics = {}\n",
    "\n",
    "    for error_rate in error_rates:\n",
    "        metrics = []\n",
    "\n",
    "        for capacity in capacities:\n",
    "            filters = harness.build_bloom_filters(\n",
    "                f\"cap{capacity}-fp{error_rate}\",\n",
    "                tqdm(train_dataloader.dataset),\n",
    "                expected_items=capacity,\n",
    "                fp_rate=error_rate\n",
    "            )\n",
    "\n",
    "            stages = RecsPipelineStages(\n",
    "                filtering = [\n",
    "                    BloomFilter(filters),\n",
    "                    CandidatePadding(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            pipeline = template.build(overrides=stages)\n",
    "\n",
    "            m = model.compute_validation_metrics(\n",
    "                build_prediction_fn(pipeline, train_dataloader),\n",
    "                tqdm(val_dataloader),\n",
    "                harness.num_recs\n",
    "            )\n",
    "            metrics.append((capacity, m))\n",
    "        filter_metrics[error_rate] = metrics\n",
    "        \n",
    "    return filter_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = [100, 1000, 10000]\n",
    "error_rates = [0.1, 0.01, 0.001]\n",
    "\n",
    "bloom_filter_metrics = sweep_filter_params(\n",
    "    harness.models[\"bce\"],\n",
    "    harness.templates[\"bce\"][\"improved-retrieval\"],\n",
    "    capacities,\n",
    "    error_rates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = [100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_plot_capacities = np.array(capacities)\n",
    "filtering_plot_recalls = np.array([[m[1]['recall'].cpu().item() for m in bloom_filter_metrics[fp]] for fp in error_rates])\n",
    "filtering_plot_ndcgs = np.array([[m[1]['ndcg'].cpu().item() for m in bloom_filter_metrics[fp]] for fp in error_rates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=1, dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.plot(filtering_plot_capacities, filtering_plot_recalls[0], label=\"FP Rate=0.1\")\n",
    "plt.plot(filtering_plot_capacities, filtering_plot_recalls[1], label=\"FP Rate=0.01\")\n",
    "plt.plot(filtering_plot_capacities, filtering_plot_recalls[2], label=\"FP Rate=0.001\")\n",
    "plt.hlines(0.2230, filtering_plot_capacities[0], filtering_plot_capacities[-1], colors='k', linestyles='dashed', label='Ideal (Estimated)')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Filter Capacity\")\n",
    "plt.ylabel(\"Recall@100\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_bytes(capacity, error_rate):\n",
    "    num_hashes = max(math.floor(math.log2(1 / error_rate)), 1)\n",
    "    bits_per_hash = math.ceil(\n",
    "                capacity * abs(math.log(error_rate)) /\n",
    "                (num_hashes * (math.log(2) ** 2)))\n",
    "    num_bits = max(num_hashes * bits_per_hash,128)\n",
    "    return num_bits//8\n",
    "\n",
    "def compute_kbytes(capacity, error_rate):\n",
    "    return compute_bytes(capacity, error_rate)/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloom_filter_sizes = {}\n",
    "\n",
    "error_rates = [0.1, 0.01, 0.001]\n",
    "capacities = [100, 300, 500, 1000, 3000, 5000, 10000]\n",
    "\n",
    "for error_rate in error_rates:\n",
    "    filter_sizes = []\n",
    "    \n",
    "    for capacity in capacities:\n",
    "        size = compute_kbytes(capacity, error_rate)\n",
    "        filter_sizes.append((capacity, size))\n",
    "    bloom_filter_sizes[error_rate] = filter_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_plot_capacities = np.array(capacities)\n",
    "filtering_plot_sizes = np.array([[s[1] for s in bloom_filter_sizes[fp]] for fp in error_rates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=1, dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.plot(filtering_plot_capacities, filtering_plot_sizes[0], label=\"FP Rate=0.1\")\n",
    "plt.plot(filtering_plot_capacities, filtering_plot_sizes[1], label=\"FP Rate=0.01\")\n",
    "plt.plot(filtering_plot_capacities, filtering_plot_sizes[2], label=\"FP Rate=0.001\")\n",
    "plt.hlines(3.5, filtering_plot_capacities[0], filtering_plot_capacities[-1], colors='lightgray', linestyles='dashed', label='3.5kB Budget')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Filter Capacity (Items)\")\n",
    "plt.ylabel(\"Filter Size (kBytes)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Baseline For Further Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.build_bloom_filters(\n",
    "    \"cap5000-fp0.1\"\n",
    "    tqdm(train_dataloader.dataset),\n",
    "    expected_items=5000,\n",
    "    fp_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_template(\"bce\", \"improved-filtering\",\n",
    "    RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(),\n",
    "        ],\n",
    "        filtering = [\n",
    "            BloomFilter(),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            MatrixFactorizationScoring(),\n",
    "        ],\n",
    "        ordering = [\n",
    "            DitheredOrdering(epsilon=dithering_eps),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-evaluate Idealized Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_idealized_stages(\"bce\", \"improved-filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage_name in [\"retrieval\", \"filtering\", \"scoring\", \"ordering\"]:\n",
    "        print(f\"With idealized {stage_name}\")\n",
    "        print(\"==============================\")\n",
    "        print_metrics(harness.metrics[\"bce\"][f\"improved-retrieval-with-ideal-{stage_name}\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced dithering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_range(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "ordering_epsilons = list(float_range(1.0,3.75,0.25))\n",
    "\n",
    "dithering_metrics = []\n",
    "\n",
    "for epsilon in ordering_epsilons:\n",
    "    harness.create_pipeline(\n",
    "        \"warp\", f\"improved-ordering-{epsilon}\", template_name=\"improved-filtering\",\n",
    "        stages = RecsPipelineStages(\n",
    "            ordering = [\n",
    "                DitheredOrdering(epsilon=epsilon),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dithering_pipeline = harness.pipelines[\"warp\"][f\"improved-ordering-{epsilon}\"]\n",
    "    dithering_pipeline.caching = True\n",
    "    \n",
    "    m = harness.evaluate_pipeline(\n",
    "        \"warp\", f\"improved-ordering-{epsilon}\", train_dataloader, tqdm(val_dataloader)\n",
    "    )\n",
    "    \n",
    "    initial_results = dithering_pipeline.cache\n",
    "    dithering_pipeline.cache = {}\n",
    "    \n",
    "    for user_id in tqdm(initial_results.keys()):\n",
    "        user_recs = copy.deepcopy(initial_results[user_id])\n",
    "        user_recs = dithering_pipeline.components[-1].run(user_recs)\n",
    "        dithering_pipeline.cache[user_id] = user_recs\n",
    "\n",
    "    rerun_results = dithering_pipeline.cache\n",
    "    \n",
    "    dithering_pipeline.caching = False\n",
    "    dithering_pipeline.cache = {}\n",
    "    \n",
    "    overlaps = []\n",
    "\n",
    "    for user_id in tqdm(initial_results.keys()): \n",
    "        _, initial_indices = th.topk(initial_results[user_id].scores, harness.num_recs)\n",
    "        _, rerun_indices = th.topk(rerun_results[user_id].scores, harness.num_recs)\n",
    "\n",
    "        intersection = len(np.intersect1d(initial_indices, rerun_indices))\n",
    "        overlaps.append(intersection)\n",
    "        \n",
    "    initial_results = None\n",
    "    rerun_results = None\n",
    "        \n",
    "    m['median_overlap'] = np.median(np.array(overlaps))\n",
    "    m['mean_overlap'] = np.mean(np.array(overlaps))\n",
    "    m['min_overlap'] = np.min(np.array(overlaps))\n",
    "    m['max_overlap'] = np.max(np.array(overlaps))\n",
    "    \n",
    "    dithering_metrics.append((epsilon, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dithering_plot_epsilons = np.array([m[0] for m in dithering_metrics])\n",
    "dithering_plot_ndcgs = np.array([m[1]['ndcg'].cpu().item() for m in dithering_metrics])\n",
    "dithering_plot_overlaps = np.array([m[1]['median_overlap'] for m in dithering_metrics])\n",
    "dithering_plot_novel_items = np.array([100 - m[1]['median_overlap'] for m in dithering_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=2, dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.plot(dithering_plot_epsilons, dithering_plot_ndcgs)\n",
    "plt.xlabel(\"Dithering Epsilon\")\n",
    "plt.ylabel(\"NDCG\")\n",
    "plt.axvspan(1.5, 3.0, color='green', alpha=0.1, label=\"Typical Range\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=2, dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.plot(dithering_plot_epsilons, dithering_plot_overlaps)\n",
    "plt.xlabel(\"Dithering Epsilon\")\n",
    "plt.ylabel(\"Overlap (# of Items)\")\n",
    "plt.axvspan(1.5, 3.0, color='green', alpha=0.1, label=\"Typical Range\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=2, dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.plot(dithering_plot_overlaps, dithering_plot_ndcgs)\n",
    "plt.xlabel(\"Overlap (# of Items)\")\n",
    "plt.ylabel(\"NDCG\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Baseline For Further Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dithering_eps = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_template(\"bce\", \"improved-ordering\",\n",
    "    RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(),\n",
    "        ],\n",
    "        filtering = [\n",
    "            BloomFilter(),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(),\n",
    "            MatrixFactorizationScoring(),\n",
    "        ],\n",
    "        ordering = [\n",
    "            DitheredOrdering(epsilon=adj_dithering_eps),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"bce\", \"improved-ordering\", template_name=\"improved-ordering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    \"bce\", \"improved-ordering\", train_dataloader, tqdm(val_dataloader)\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_idealized_stages(\"bce\", \"improved-ordering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage_name in [\"retrieval\", \"filtering\", \"scoring\", \"ordering\"]:\n",
    "        print(f\"With idealized {stage_name}\")\n",
    "        print(\"==============================\")\n",
    "        print_metrics(harness.metrics[\"bce\"][f\"improved-ordering-with-ideal-{stage_name}\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model With BPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.load_model(\"bpr\", \"../models/pious-meadow-467-bpr.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Raw Model-Only Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = harness.evaluate_model(\"bpr\", tqdm(val_dataloader))\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Nearest Neighbor Search Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.build_ann_indices(\"bpr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate The End-To-End Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_template(\"bpr\", \"improved-ordering\",\n",
    "    RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(harness.models[\"bce\"]),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(harness.indices[\"bce\"][\"approx\"]),\n",
    "        ],\n",
    "        filtering = [\n",
    "            BloomFilter(harness.filters[\"cap5000-fp0.1\"]),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"bce\"]),\n",
    "            MatrixFactorizationScoring(harness.models[\"bce\"]),\n",
    "        ],\n",
    "        ordering = [\n",
    "            DitheredOrdering(epsilon=adj_dithering_eps),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"bpr\", \"improved-scoring\", template_name=\"improved-ordering\",\n",
    "    stages = RecsPipelineStages(\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"bpr\"]),\n",
    "            MatrixFactorizationScoring(harness.models[\"bpr\"]),\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    \"bpr\", \"improved-scoring\", train_dataloader, tqdm(val_dataloader)\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"bpr\", \"improved-scoring-retrieval\", template_name=\"improved-ordering\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"bpr\"]),\n",
    "            ANNSearch(harness.indices[\"bpr\"][\"approx\"]),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"bpr\"]),\n",
    "            MatrixFactorizationScoring(harness.models[\"bpr\"]),\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    \"bpr\", \"improved-scoring-retrieval\", train_dataloader, tqdm(val_dataloader)\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model With WARP Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.load_model(\"warp\", \"../models/good-sweep-1-warp-01.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Nearest Neighbor Search Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.build_ann_indices(\"warp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Raw Model-Only Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = harness.evaluate_model(\"warp\", tqdm(val_dataloader))\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate The End-To-End Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_template(\"warp\", \"improved-ordering\",\n",
    "    RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(harness.models[\"bce\"]),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(ann_indices[\"bce\"][\"approx\"]),\n",
    "        ],\n",
    "        filtering = [\n",
    "            BloomFilter(bloom_filters[\"cap5000-fp0.1\"]),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"bce\"]),\n",
    "            MatrixFactorizationScoring(harness.models[\"bce\"]),\n",
    "        ],\n",
    "        ordering = [\n",
    "            DitheredOrdering(harness.num_candidates, harness.num_recs, epsilon=dithering_eps),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"warp\", \"improved-scoring\", template_name=\"improved-ordering\",\n",
    "    stages = RecsPipelineStages(\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"warp\"]),\n",
    "            MatrixFactorizationScoring(harness.models[\"warp\"]),\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    \"warp\", \"improved-scoring\", train_dataloader, tqdm(val_dataloader)\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"warp\", \"improved-scoring-retrieval\", template_name=\"improved-ordering\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(harness.models[\"warp\"]),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(ann_indices[\"warp\"][\"approx\"]),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"warp\"]),\n",
    "            MatrixFactorizationScoring(harness.models[\"warp\"]),\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = harness.evaluate_pipeline(\n",
    "    \"warp\", \"improved-scoring-retrieval\", train_dataloader, tqdm(val_dataloader)\n",
    ")\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Baseline For Further Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_template(\"warp\", \"improved-scoring\",\n",
    "    RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            ItemEmbeddingsFetcher(harness.models[\"warp\"]),\n",
    "            UseItemEmbeddingsAsUserEmbeddings(),\n",
    "            ANNSearch(ann_indices[\"warp\"][\"approx\"]),\n",
    "        ],\n",
    "        filtering = [\n",
    "            BloomFilter(bloom_filters[\"cap5000-fp0.1\"]),\n",
    "            CandidatePadding(),\n",
    "        ],\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"warp\"]),\n",
    "            MatrixFactorizationScoring(harness.models[\"warp\"]),\n",
    "        ],\n",
    "        ordering = [\n",
    "            DitheredOrdering(epsilon=adj_dithering_eps),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-evaluate Idealized Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sort out how to handle creating idealized stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"warp\", \"ideal-retrieval\", template_name=\"improved-scoring\",\n",
    "    stages = RecsPipelineStages(\n",
    "        retrieval = [\n",
    "            UserEmbeddingFetcher(harness.models[\"warp\"]),\n",
    "            IdealizedANNSearch(val_dataloader.dataset, ann_indices[\"warp\"][\"exact\"]),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"warp\", \"ideal-filtering\", template_name=\"improved-scoring\",\n",
    "    stages = RecsPipelineStages(\n",
    "        filtering = [\n",
    "            IdealizedFilter(train_dataloader.dataset),\n",
    "            CandidatePadding(),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"warp\", \"ideal-scoring\", template_name=\"improved-scoring\",\n",
    "    stages = RecsPipelineStages(\n",
    "        scoring = [\n",
    "            # Re-fetching user avg embeddings keeps retrieval changes from affecting scoring\n",
    "            UserAvgEmbeddingFetcher(harness.models[\"warp\"]),\n",
    "            IdealizedMatrixFactorizationScoring(harness.models[\"warp\"], val_dataloader.dataset),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.create_pipeline(\"warp\", \"ideal-ordering\", template_name=\"improved-scoring\",\n",
    "    stages = RecsPipelineStages(\n",
    "        ordering = []\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_idealized_stages(\"warp\", \"improved-scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_idealized_stages(\"warp\", \"improved-scoring\")\n",
    "for stage_name in [\"retrieval\", \"filtering\", \"scoring\", \"ordering\"]:\n",
    "        print(f\"With idealized {stage_name}\")\n",
    "        print(\"==============================\")\n",
    "        print_metrics(harness.metrics[\"warp\"][f\"improved-scoring-with-ideal-{stage_name}\"])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move filtering before scoring\n",
    "# TODO: Compare pipeline-only improvements to model-only improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try injecting most popular items into the candidate set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
